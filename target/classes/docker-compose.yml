version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    # 配置好 docker 内的假域名
    hostname: namenode
    container_name: namenode
    ports:
      - 9000:9000
      - 50070:50070
    restart: always
    # 此处使用 network的模式 是host，意思是基于本地的hosts文件内容，此处用处在于和spark集群集成，以及便于外部访问随机生成的端口号
    network_mode: 'host'
    volumes:
      - ./hadoop/conf:/opt/hadoop-3.2.1/etc/hadoop/conf
      - ./hadoop/namenode:/hadoop/dfs/name
      - ./hadoop/input_files:/input_files
    # environment参数可以添加在 env_file 中
    environment:
      - CLUSTER_NAME=test
      # 配置 hdfs 用户权限问题，不需要只允许 hadoop 用户访问
      - HDFS_CONF_dfs_permissions=false
    # env_file中的参数可以配置在env_file
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    hostname: resourcemanager
    container_name: resourcemanager
    ports:
      - 8030:8030
      - 8031:8031
      - 8032:8032
      - 8033:8033
      - 8088:8088
    restart: always
    network_mode: 'host'
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    hostname: historyserver
    container_name: historyserver
    ports:
      - 8188:8188
    restart: always
    network_mode: 'host'
    depends_on:
      - namenode
      - datanode1
      - datanode2
    volumes:
      - ./hadoop/historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    hostname: nodemanager1
    container_name: nodemanager1
    ports:
      - 8040:8040
      - 8041:8041
      - 8042:8042
    restart: always
    network_mode: 'host'
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode1
    container_name: datanode1
    restart: always
    network_mode: 'host'
    environment:
      # 等价于在 hdfs-site.xml 中配置 dfs.datanode.address
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:50010
      # dfs.datanode.ipc.address 不使用默认端口的意义是在同一机器起多个 datanode，暴露端口需要不同
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:50020
      # dfs.datanode.http.address
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:50075
    ports:
      - 50010:50010
      - 50020:50020
      - 50075:50075
    depends_on:
      - namenode
    volumes:
      - ./hadoop/datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode2
    container_name: datanode2
    restart: always
    network_mode: 'host'
    environment:
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:50012
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:50022
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:50072
    ports:
      - 50012:50012
      - 50022:50022
      - 50072:50072
    depends_on:
      - namenode
    volumes:
      - ./hadoop/datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode3
    container_name: datanode3
    restart: always
    network_mode: 'host'
    environment:
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:50013
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:50023
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:50073
    ports:
      - 50013:50013
      - 50023:50023
      - 50073:50073
    depends_on:
      - namenode
    volumes:
      - ./hadoop/datanode3:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
